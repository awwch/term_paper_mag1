# -*- coding: utf-8 -*-
"""
Created on Mon Jul 25 12:53:41 2016

@author: cogntech
"""
import os
import nltk
from nltk.corpus import stopwords
import pymorphy2
import pandas as pd
import re
from collections import Counter

def str_key (_dict):
    key = (str(list(_dict.keys())).strip('[]')).replace("'",'')
    return(key)
def str_value (_dict): 
    value = (str(list(_dict.values())).strip('[]')).replace("'",'')
    return(value)

stop_words = stopwords.words('russian')
stop_words.extend(["тка","ка","например", "также", "нибудь", "который", "свой", "обычно", "некоторый", "кому"])
morph = pymorphy2.MorphAnalyzer()
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
_dict = pd.read_csv(os.getcwd()+"\\ozhegov2_ex.csv", header=0, delimiter=";")

def dict_to_wordlist(text, remove_stopwords=True):
    words = []
    gram_info = []
    text = re.sub("[^а-яА-Я]"," ", text)
    all_words = text.lower().split()
    for word in all_words:
        p = morph.parse(word)[0]
        if word not in stop_words and  p.normal_form not in stop_words:
            words.append({word:p.normal_form})
            gram_info.append({p.normal_form:str(p.tag)})
    return(words,gram_info)
    
def dict_to_sentences(text,tokenizer, remove_stopwords=True):
    raw_sentences = tokenizer.tokenize(text.strip())
    sentences = []
    for raw_sentence in raw_sentences:
        #if len(raw_sentence) > 0:
        if dict_to_wordlist(raw_sentence,remove_stopwords) not in sentences:
            sentences.append(dict_to_wordlist(raw_sentence,remove_stopwords))
    return(sentences)

def lemmas_in_def(lemmas,sentences):
    c = 0
    lemma_counter = []
    for lemma in lemmas:
        for sentence in sentences:
            m = re.search(lemma,str(sentence))
            if m != None:
                c += 1
        lemma_count = {lemma:c}
        lemma_counter.append(lemma_count)
        c = 0  
    return(lemma_counter)
    
sentences = []
baseforms = []
words = []
i = 0
for art in _dict["DEF"]:
    words.append(_dict["VOCAB"][i])
    art = _dict["VOCAB"][i]+ ' ' + str(art)
    baseform = _dict["BASEFORM"][i]
    baseforms.append(baseform)
    sentences.append(dict_to_sentences(str(art), tokenizer))
    i += 1      

om_counter = []
for i in range(len(baseforms)):
    if baseforms[i] >= 1:
        om_counter.append(int(baseforms[i]))
    else:
        om_counter.append(0) 


omonymy = []
for i in range(len(om_counter)):
    omonymy.append({words[i]:om_counter[i]})
w_counter = []
for word in words:
    c = words.count(word) 
    w_counter.append({word:c})
############
sim_w = []
for w in w_counter:
    if int(str_value(w)) >= 1 and w not in sim_w:
        sim_w.append(w)
############
fine_om = []
for l in range(len(sim_w)+1):
    for i in range(len(omonymy)):
        if str_key(omonymy[i]) == str_key(sim_w[l]) and str_value(omonymy[i]) != '1':
            if int(str_value(omonymy[i+1])) < int(str_value(omonymy[i])):                
                fine_om.append({str_key(omonymy[i]):int(str_value(omonymy[i]))})

for i in range(len(fine_om)):
    for o in omonymy:
        if str_key(o) == str_key(fine_om[i]):
            omonymy.remove(o)
            omonymy.append(fine_om[i])
######очистить########
f = open('Ozhegov_homonymy.csv','w',encoding = 'utf-8')
for o in omonymy:
    f.write(str(o)+'\n')        
f.close()        


#lemmas = []
#normalized_forms = []
#normalized_gram = []
#nouns = []
#inf = []
#for sentence in sentences:
    #for s in sentence:
     #   try:
      #      if len(s) == 0:
       #         sentences.remove(sentence)
        #except ValueError:
         #   continue
#    normalized_gram.append(sentence[-1])
 #   for word in sentence[-1]:
  #      word = str_key(word)        
   #     normalized_forms.append(word)
    #    if word not in lemmas:
     #       lemmas.append(word)
            
#for forms in normalized_gram:
 #   if forms == []:
  #      normalized_gram.remove(forms)
   # for word in forms:
    #    line = str_value(word)
     #   m = re.search('NOUN',line)
      #  if m != None:
       #     nouns.append(str_key(word))
        #m = re.search('INFN',line)
        #if m != None:
         #   inf.append(str_key(word))

            
#c = Counter(nouns).most_common()
#v = Counter(inf).most_common()

### ГИПЕРОНИМЫ В ТОЛКОВАНИИ
#for _class in c[:20]:
 #   #дубли
  #  f = open(_class[0]+'.txt','a',encoding = 'utf-8')    
   # for sentence in sentences:
    #    m = re.search(_class[0] + '.*NOUN.*nomn', str(sentence[-1])) #регулярка неоч
     #   if m != None:
      #      for s in sentence:
       #         f.write(str_key(s[0]) + '\n')
    #f.close()

### 1-й глагол +(или) 1-е сущ
#for verb in v[:20]:
 #   f=  open(verb[0] +'.txt','a',encoding = 'utf-8')
  #  for sentence in sentences:
   #     m = re.search(verb[0], str(sentence[-1]))
    #    
     #   if m != None:
      #      for s in sentence:
       #         f.write(str_key(s[0]) + '\n')            
    #f.close()
