# -*- coding: utf-8 -*-
"""
Created on Mon Jul 25 12:53:41 2016

@author: cogntech
"""
import os
import nltk
from nltk.corpus import stopwords
import pymorphy2
import pandas as pd
import re
from collections import Counter

def str_key (_dict):
    key = (str(list(_dict.keys())).strip('[]')).replace("'",'')
    return(key)
def str_value (_dict): 
    value = (str(list(_dict.values())).strip('[]')).replace("'",'')
    return(value)

stop_words = stopwords.words('russian')
stop_words.extend(["тка","ка","например", "также", "нибудь", "который", "свой", "обычно", "некоторый", "кому"])
morph = pymorphy2.MorphAnalyzer()
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
_dict = pd.read_csv(os.getcwd()+"\\ozhegov2_ex.csv", header=0, delimiter=";")

def dict_to_wordlist(text, remove_stopwords=True):
    words = []
    gram_info = []
    text = re.sub("[^а-яА-Я]"," ", text)
    all_words = text.lower().split()
    for word in all_words:
        p = morph.parse(word)[0]
        if word not in stop_words and  p.normal_form not in stop_words:
            words.append({word:p.normal_form})
            gram_info.append({p.normal_form:str(p.tag)})
    return(words,gram_info)
    
def dict_to_sentences(text,tokenizer, remove_stopwords=True):
    raw_sentences = tokenizer.tokenize(text.strip())
    sentences = []
    for raw_sentence in raw_sentences:
        #if len(raw_sentence) > 0:
        if dict_to_wordlist(raw_sentence,remove_stopwords) not in sentences:
            sentences.append(dict_to_wordlist(raw_sentence,remove_stopwords))
    return(sentences)

def lemmas_in_def(lemmas,sentences):
    c = 0
    lemma_counter = []
    for lemma in lemmas:
        for sentence in sentences:
            m = re.search(lemma,str(sentence))
            if m != None:
                c += 1
        lemma_count = {lemma:c}
        lemma_counter.append(lemma_count)
        c = 0  
    return(lemma_counter)
    
sentences = []
baseforms = []
words = []
i = 0
for art in _dict["DEF"]:
    words.append(_dict["VOCAB"][i])
    art = _dict["VOCAB"][i]+ ' ' + str(art)
    baseforms.append(_dict["BASEFORM"][i])
    sentences.append(dict_to_sentences(str(art), tokenizer))
    i += 1      

om_counter = []
for i in range(len(baseforms)):
    if baseforms[i] >= 1:
        om_counter.append(int(baseforms[i]))
    else:
        om_counter.append(0) 

omonymy = []
for i in range(len(om_counter)):
    omonymy.append({words[i]:om_counter[i]})
w_counter = []
for word in words:
    c = words.count(word)
    if {word:c} not in w_counter:    
        w_counter.append({word:c})
    
sim_w = []
for w in w_counter:
    if int(str_value(w)) >= 1 and w not in sim_w:
        sim_w.append(w)
fine_om = []
for l in range(len(sim_w)+1):
    for i in range(len(omonymy)):
        if str_key(omonymy[i]) == str_key(sim_w[l]) and str_value(omonymy[i]) != '1':
            if int(str_value(omonymy[i+1])) < int(str_value(omonymy[i])):                
                fine_om.append({str_key(omonymy[i]):int(str_value(omonymy[i]))})

for i in range(len(fine_om)):
    for o in omonymy:
        if str_key(o) == str_key(fine_om[i]):
            omonymy.remove(o)
            omonymy.append(fine_om[i])

hom = []
#f = open('Homonymy.csv','w',encoding = 'utf-8')
for i in range(len(words)):
    hom.append({words[i]:om_counter[i]})
    #f.write(words[i]+';'+ str(om_counter[i]) + '\n')
#f.close()

rep_count = []
for o in omonymy:
    c = omonymy.count(o)
    if {str_key(o):c} not in rep_count:
        rep_count.append({str_key(o):c})

counter = []
counter1 = []
for i in range(len(rep_count)):
    count = int(str_value(rep_count[i]))
    for c in range(count):
        if c == 0:
            counter.append({str_key(rep_count[i]):1})
            counter1.append(1)
        else:
            counter.append({str_key(rep_count[i]):c + 1}) 
            counter1.append(c+1)
counted_words = []        
for c in counter:
    counted_words.append(str_key(c))

all_words = []
i = 0
for w in words:
    all_words.append({w:i})
    i+=1

count_indx = []
i = 0
for w in counted_words:
    count_indx.append({w:i})
    i+=1

mistakes = []
for a in all_words:
    if a not in count_indx:
        mistakes.append(a)
            
final = []
#for c in counter:
for o in omonymy:
    for c in counter:
        if str_key(o) == str_key(c):
            line = str(str_key(o) + ';' + str_value(o) + ';' + str_value(c))
            if line not in final:
                final.append(line)
#for o in omonymy:
    
                
f = open('Homonymy.csv','w',encoding = 'utf-8')
for line in final:
    f.write(line+ '\n')
            #f.write(str_key(c) + ';' + str_value(omonymy) + ';' + str_value(counter) + 'n\')
              
#c = Counter(nouns).most_common()
#v = Counter(inf).most_common()

### ГИПЕРОНИМЫ В ТОЛКОВАНИИ
#for _class in c[:20]:
 #   #дубли
  #  f = open(_class[0]+'.txt','a',encoding = 'utf-8')    
   # for sentence in sentences:
    #    m = re.search(_class[0] + '.*NOUN.*nomn', str(sentence[-1])) #регулярка неоч
     #   if m != None:
      #      for s in sentence:
       #         f.write(str_key(s[0]) + '\n')
    #f.close()

### 1-й глагол +(или) 1-е сущ
#for verb in v[:20]:
 #   f=  open(verb[0] +'.txt','a',encoding = 'utf-8')
  #  for sentence in sentences:
   #     m = re.search(verb[0], str(sentence[-1]))
    #    
     #   if m != None:
      #      for s in sentence:
       #         f.write(str_key(s[0]) + '\n')            
    #f.close()
